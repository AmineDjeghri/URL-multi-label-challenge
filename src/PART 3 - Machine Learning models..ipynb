{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 - Machine Learning models\n",
    "#### Dans cette partie, nous allons diviser nos données, créer des features (OneHot, TF-IDF) et construire nos modèles (Baselines et modèles améliorés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocess, split_data\n",
    "from utils import display_score\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00004-1b8fcd71-6348-4510-a9dc-bdd7dcf82f2d-c000.snappy.parquet\n",
      "part-00003-1b8fcd71-6348-4510-a9dc-bdd7dcf82f2d-c000.snappy.parquet\n",
      "part-00001-1b8fcd71-6348-4510-a9dc-bdd7dcf82f2d-c000.snappy.parquet\n",
      "part-00002-1b8fcd71-6348-4510-a9dc-bdd7dcf82f2d-c000.snappy.parquet\n",
      "part-00000-1b8fcd71-6348-4510-a9dc-bdd7dcf82f2d-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "parquet_data_path = \"../data/\"\n",
    "preprocess = Preprocess()\n",
    "df = preprocess.create_dataframe(parquet_data_path, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>day</th>\n",
       "      <th>sous_domaine</th>\n",
       "      <th>domaine</th>\n",
       "      <th>top_domaine</th>\n",
       "      <th>tokens_path</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>1001</th>\n",
       "      <th>1002</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[329, 1234, 5183, 96, 377]</td>\n",
       "      <td>4</td>\n",
       "      <td>www</td>\n",
       "      <td>societe</td>\n",
       "      <td>com</td>\n",
       "      <td>societ madam karin pinchon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[158, 650, 1175, 831, 953]</td>\n",
       "      <td>4</td>\n",
       "      <td>www</td>\n",
       "      <td>ebay-kleinanzeigen</td>\n",
       "      <td>de</td>\n",
       "      <td>nu fbaumstamm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[325, 253, 1775, 640, 543]</td>\n",
       "      <td>4</td>\n",
       "      <td>psychologie</td>\n",
       "      <td>aufeminin</td>\n",
       "      <td>com</td>\n",
       "      <td>forum copain troubl comport fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1143, 210, 531, 18, 41]</td>\n",
       "      <td>4</td>\n",
       "      <td>fr.shopping</td>\n",
       "      <td>rakuten</td>\n",
       "      <td>com</td>\n",
       "      <td>poweron pr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1171, 1071, 1192, 1533, 1277]</td>\n",
       "      <td>4</td>\n",
       "      <td>www</td>\n",
       "      <td>cdiscount</td>\n",
       "      <td>com</td>\n",
       "      <td>search coqu samsung galaxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67590</th>\n",
       "      <td>[1193, 318, 22, 1107, 1111]</td>\n",
       "      <td>12</td>\n",
       "      <td>www</td>\n",
       "      <td>senscritique</td>\n",
       "      <td>com</td>\n",
       "      <td>ser hard critiqu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67591</th>\n",
       "      <td>[997, 1598, 993, 992, 984]</td>\n",
       "      <td>13</td>\n",
       "      <td>www</td>\n",
       "      <td>cdiscount</td>\n",
       "      <td>com</td>\n",
       "      <td>pret port vet blaz femm top manch longu vest f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67592</th>\n",
       "      <td>[1193, 1410, 1049, 1187, 358]</td>\n",
       "      <td>13</td>\n",
       "      <td>www</td>\n",
       "      <td>programme-tv</td>\n",
       "      <td>net</td>\n",
       "      <td>programm autr laventur robinson laventur robinson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67593</th>\n",
       "      <td>[63, 5184, 1254, 1119]</td>\n",
       "      <td>12</td>\n",
       "      <td>fr</td>\n",
       "      <td>windfinder</td>\n",
       "      <td>com</td>\n",
       "      <td>forecast point roug</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67594</th>\n",
       "      <td>[5154, 864, 182, 1048]</td>\n",
       "      <td>13</td>\n",
       "      <td>www</td>\n",
       "      <td>newsbomb</td>\n",
       "      <td>gr</td>\n",
       "      <td>kosmos story den tha pisteyet tin iliki aytis ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67595 rows × 1909 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               target  day sous_domaine             domaine  \\\n",
       "0          [329, 1234, 5183, 96, 377]    4          www             societe   \n",
       "1          [158, 650, 1175, 831, 953]    4          www  ebay-kleinanzeigen   \n",
       "2          [325, 253, 1775, 640, 543]    4  psychologie           aufeminin   \n",
       "3            [1143, 210, 531, 18, 41]    4  fr.shopping             rakuten   \n",
       "4      [1171, 1071, 1192, 1533, 1277]    4          www           cdiscount   \n",
       "...                               ...  ...          ...                 ...   \n",
       "67590     [1193, 318, 22, 1107, 1111]   12          www        senscritique   \n",
       "67591      [997, 1598, 993, 992, 984]   13          www           cdiscount   \n",
       "67592   [1193, 1410, 1049, 1187, 358]   13          www        programme-tv   \n",
       "67593          [63, 5184, 1254, 1119]   12           fr          windfinder   \n",
       "67594          [5154, 864, 182, 1048]   13          www            newsbomb   \n",
       "\n",
       "      top_domaine                                        tokens_path  100  \\\n",
       "0             com                         societ madam karin pinchon    0   \n",
       "1              de                                      nu fbaumstamm    0   \n",
       "2             com                     forum copain troubl comport fd    0   \n",
       "3             com                                         poweron pr    0   \n",
       "4             com                         search coqu samsung galaxy    0   \n",
       "...           ...                                                ...  ...   \n",
       "67590         com                                   ser hard critiqu    0   \n",
       "67591         com  pret port vet blaz femm top manch longu vest f...    0   \n",
       "67592         net  programm autr laventur robinson laventur robinson    0   \n",
       "67593         com                                forecast point roug    0   \n",
       "67594          gr  kosmos story den tha pisteyet tin iliki aytis ...    0   \n",
       "\n",
       "       1000  1001  1002  ...  990  991  992  993  994  995  996  997  998  999  \n",
       "0         0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "1         0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "2         0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "3         0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "4         0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "...     ...   ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "67590     0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "67591     0     0     0  ...    0    0    1    1    0    0    0    1    0    0  \n",
       "67592     0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "67593     0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "67594     0     0     0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[67595 rows x 1909 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trois approches peuvent etre prises pour la colonne sous_domaine:\n",
    "- La prétraiter (enlever les www et les lettres, tokenizer...) et l'ajouter à la colonne path pour former une description\n",
    "- L'enlever \n",
    "- La prétraiter afin de construire une feature catégorielle.\n",
    "\n",
    "Pour l'instant nous allons l'enlever\n",
    "\n",
    "## Construction des features à partir du path\n",
    "Différentes approches peuvent etre utilisées sur la colonne path:\n",
    "#### L'utilisation de TF-IDF \n",
    "- TF-IDF: refléte l'importance d'un mot pour un document dans une collection (corpus) mais ne prend pas en compte le sens sémantique des mots. TF signifie la probabilité d'occurrence d'un mot dans une phrase.\n",
    "- TF-IDF donne plus d'importance aux mots qui apparaissent moins fréquemment dans l'ensemble du corpus et donne également de l'importance aux mots les plus fréquents qui apparaissent dans chaque donnée.\n",
    "\n",
    "Nous testerons les featurizations suivantes:\n",
    "- TF-IDF: unigrams, bigrams, trigrams et word n-grams.\n",
    "- TF-IDF based character: unigrams, bigrams, trigrams. (considérer une séquence de caractères plutôt qu'une séquence de mots)\n",
    "\n",
    "\n",
    "#### L'utilisation des Embeddings (word2vec)\n",
    "- Word2vec est l'un des modèle de l'état de l'art pour les embeddings.  Il permet de convertir du texte en vecteurs numériques tout en préservantles relations sémantiques entre les mots.\n",
    "- vecteur de 300 dimensions \n",
    "\n",
    "#### L'utilisation d'une combinaison de TF-IDF et moyenne des Embeddings:\n",
    "Nombiner les n-grammes de caractères avec les vecteurs obtenus avec Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50696, 6)\n",
      "(16899, 6)\n",
      "Index(['day', 'domaine', 'top_domaine', 'tokens_path'], dtype='object')\n",
      "Index(['day', 'domaine', 'top_domaine', 'tokens_path'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50696, 9216)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, target_train, target_test = split_data(df, test_size=0.25)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50696, 1903)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction de la dimension des labels\n",
    "Nous avons 1903 labels pour notre target ce qui reprensente un nombre assez elevé pour de la classification multi-label.\n",
    "Nous allons donc effectuer une réduction sur l'espace de dimension des labels.\n",
    "Plusieurs algorithmes existent pour la réduction de l'espace des labels: compressed sensing (CS) et la  Principal Label Space Transformation (PLST) qui est l'équivalente du PCA sur les features. Egalement des techniques d'embeddings entre targets\n",
    "Contrairement à ces techniques qui sont indépendentes de nos entrées, il en existe aussi d'autres qui le sont comme par exemple la CPLST ( Conditional Principal Label Space Transformation ) \n",
    "\n",
    "#### PLST :\n",
    "PLST or Principal Label Space Transformation est une méthode de réduction de dimensionnalité utilisée explicitement pour les labels des datasets et permettant de les représenter géométriquement tout en minimisant l'erreur de reconstruction, nous combinons cette méthode ensuite avec des baselines modèles pour l'apprentissage.\n",
    "  \n",
    "\n",
    "Un autre moyen serait d'essayer d'utiliser une approches par embedding sur les labels afin d'extraire les labels qui sont souvent ensemble.\n",
    "\n",
    "#### Label Graph :\n",
    "C'est une méthode permettant de représenter un embedding des différents labels de l'output en créant ainsi un graphe de lien représentations des interactions continues entre les différents labels attendues. C'est un modèle qui a démontré dans la littérature une accuracy élevé et des résultats très concluants dans le domaine de la classification multi-label.\n",
    "\n",
    "\n",
    "Pour la classification multi-label, il existe une librarie nommée `scikit-multilearn` qui s'appuie sur la librarie scikit-learn. Elle contient aussi un wrapper autour de  MEKA qui propose une implémentation des méthodes d'apprentissage et d'évaluation multi-labels comme la PLST et le LABELGRaph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des modèles :\n",
    "#### Baseslines: \n",
    "- Dans un premier temps, nous allons construire nos modèles Baselines avec l'ensemble des targets présentes dans notre dataset. Chaque modèle essaiera de prédire les targets sur soit le nombre total de 1009 targets ( Ce qui n'est peut etre pas une bonne approche), ou sur la réduction de dimension des 1009 targets. \n",
    "- Après avoir construit les baslines models, nous essaierons d'améliorer les modèles avec des hyperparamètres pour voir s'il y a une augmentation du score F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vu le peu de temps que j'ai en soirée après la journée au stage, j'ai préféré au lieu de me focaliser sur l'amélioration d'une seule approche, me documenter sur d'autres approches parallelement à l'entrainement des modèles et essayer de les implementer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import (KNeighborsClassifier,\n",
    "                               NeighborhoodComponentsAnalysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label classification\n",
    "#### 1.  OneVsRest: \n",
    "Le problème est décomposé en un problème de classification binaire multiple. Nous choisissons une classe et formons un classificateur binaire avec les échantillons de la classe sélectionnée d'un côté et tous les autres échantillons de l'autre côté. Ainsi, nous obtiendrons N classificateurs pour N étiquettes et lors du test nous classerons simplement l'échantillon comme appartenant à la classe avec le score maximum parmi les N classificateurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sans réduction de dimension des labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(class_weight='balanced',\n",
       "                                                 penalty='l1',\n",
       "                                                 solver='liblinear'),\n",
       "                    n_jobs=-1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1 = OneVsRestClassifier(LogisticRegression(penalty='l1', solver='liblinear', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = classifier1.predict(X_train)\n",
    "predictions = classifier1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier1, open(\"finalized_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = pickle.load(open(\"finalized_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3107, Recall: 0.9978, F1-measure: 0.4738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3310, Recall: 0.9760, F1-measure: 0.4681\n",
      "\n",
      "Classification Report\n",
      "test score\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2285, Recall: 0.7228, F1-measure: 0.3472\n",
      "\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1373, Recall: 0.3925, F1-measure: 0.1916\n",
      "\n",
      "Classification Report\n"
     ]
    }
   ],
   "source": [
    "print(\"train score\")\n",
    "display_score(y_train, predictions_train)\n",
    "print(\"test score\")\n",
    "display_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec réduction de dimension PCA: \n",
    "La réduction de l'espace des labels avec la PCA n'est pas la méthodes la plus appropriée car nous avons une matrice sparse et il est préférable d'appliquer la TruncatedSVD sur la les labels. Cependant, nous pourront testerons la PCA et verrons ce que ça donnera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7794021262243102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=300, random_state=42)\n",
    "pca.fit(y_train)\n",
    "\n",
    "print(pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pca = pca.transform(y_train)\n",
    "y_test_pca = pca.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = OneVsRestClassifier(LogisticRegression(penalty='l1', solver='liblinear', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = classifier1.predict(X_train)\n",
    "predictions = classifier1.predict(X_test)\n",
    "print(\"train score\")\n",
    "display_score(y_train, predictions_train)\n",
    "print(\"test score\")\n",
    "display_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduction de dimension TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(y_train)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "y_train_svd = pca.transform(y_train)\n",
    "y_test_svd = pca.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Binary Relevance:\n",
    "Transformer un problème multi-label avec K étiquettes en des problèmes de classification binaire séparés. Chaque classificateur prédit si une étiquette est présente ou non.\n",
    "Cette technique ignore les relations entre les étiquettes.\n",
    "\n",
    "Les deux techniques présentées en haut traitent le problème multi-label (choix multiples) en une série de questions oui/non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-acae8bc27a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryRelevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/skmultilearn/problem_transform/br.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0my_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             classifier.fit(self._ensure_input_format(\n\u001b[0;32m--> 162\u001b[0;31m                 X), self._ensure_output_format(y_subset))\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 213\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    434\u001b[0m             new_theta, new_sigma = self._update_mean_variance(\n\u001b[1;32m    435\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 X_i, sw_i)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_update_mean_variance\u001b[0;34m(n_past, mu, var, X, sample_weight)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mn_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mnew_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mnew_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvar\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3724\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Note that x may not be inexact and that we need it to be an array,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# not a scalar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0marrmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier, open(\"finalized_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = pickle.load(open(\"finalized_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "predictions_train = classifier.predict(X_train)\n",
    "accuracy_score(y_test,predictions)\n",
    "print('AUC ROC is {}'.format(roc_auc_score(y_test,predictions.toarray())))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"train score\")\n",
    "display_score(y_train, predictions_train)\n",
    "print(\"test score\")\n",
    "display_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Classifier Chains:\n",
    "Un classificateur 1 sera formé sur les données d'entrée. La sortie du classificateur 1 sera alimentée en entrée pour le classificateur 2, qui prédit la deuxième étiquette, la sortie du classificateur 2 sera alimentée en entrée pour le classificateur 3 et ainsi de suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print('AUC ROC is {}'.format(roc_auc_score(y_test,predictions.toarray())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
